# -*- coding: utf-8 -*-
"""
Web Content Extractor Pro - ä¸“ä¸šç½‘é¡µå†…å®¹æå–å·¥å…·
åŸºäºPyQt5å’ŒBeautifulSoupå¼€å‘ï¼Œæ”¯æŒå¤šå¹³å°å†…å®¹æå–ã€å¤šæ ¼å¼è¾“å‡ºã€æ•°å­¦å…¬å¼å¤„ç†
Version: 8.1 - CSDNæ•°å­¦å…¬å¼æ·±åº¦ä¼˜åŒ–ç‰ˆ
githubç½‘å€ï¼š https://github.com/lyp0746
QQé‚®ç®±ï¼š1610369302@qq.com
ä½œè€…ï¼šLYP

ä¼˜åŒ–é‡ç‚¹:
- CSDNæ•°å­¦å…¬å¼æ·±åº¦æ¸…ç†(ç§»é™¤XMLæ ‡ç­¾)
- Markdownæ•°å­¦å…¬å¼æ­£ç¡®æ¸²æŸ“
- PDFä¸“ä¸šä¹¦ç±é£æ ¼ä¼˜åŒ–
- æ™ºèƒ½æ–‡ä»¶å‘½å(ä½¿ç”¨æ–‡ç« æ ‡é¢˜)
- æ”¯æŒèœé¸Ÿæ•™ç¨‹ã€CSDNã€GitBookä¸‰å¤§å¹³å°

åŠŸèƒ½ç‰¹æ€§:
- æ”¯æŒå¹³å°: èœé¸Ÿæ•™ç¨‹ã€CSDNåšå®¢/ä¸“æ ã€GitBookæ–‡æ¡£
- è¾“å‡ºæ ¼å¼: Markdownã€HTMLã€PDF (ä¸“ä¸šä¹¦ç±é£æ ¼)
- æ•°å­¦å…¬å¼: å®Œæ•´LaTeX/MathJax 3.0æ”¯æŒ
- CSDNå¢å¼º: æ·±åº¦å†…å®¹æå–å’Œæ¸…ç†
- GUIä¼˜åŒ–: å¤§å­—ä½“ã€æ˜“æ“ä½œã€ä¸“ä¸šå¤–è§‚
============================================
"""
import sys
import os
import re
import warnings
from pathlib import Path
from typing import List, Tuple
from urllib.parse import urljoin, urlparse
import time
from datetime import datetime

import requests
from bs4 import BeautifulSoup, NavigableString
from PyQt5.QtWidgets import *
from PyQt5.QtCore import QThread, pyqtSignal, Qt, QSettings
from PyQt5.QtGui import QFont, QTextCursor

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings("ignore", category=DeprecationWarning)
warnings.filterwarnings("ignore", message="sipPyTypeDict")

# PDFç”Ÿæˆ - WeasyPrint 59.0+
try:
    from weasyprint import HTML, CSS
    from weasyprint.text.fonts import FontConfiguration
    WEASY_AVAILABLE = True
except ImportError:
    WEASY_AVAILABLE = False


# ======================== æ•°æ®ç»“æ„ ========================

class Article:
    """æ–‡ç« æ•°æ®ç»“æ„"""
    def __init__(self, title: str, url: str, level: int = 1):
        self.title = title
        self.url = url
        self.level = level
        self.content = ""
        self.html_content = ""
        self.author = ""
        self.date = ""
        self.category = ""


# ======================== çˆ¬è™«åŸºç±» ========================

class BaseCrawler:
    """çˆ¬è™«åŸºç±» - æä¾›é€šç”¨åŠŸèƒ½"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        }
        self.session = requests.Session()
        self.img_cache = {}
        
    def download_image(self, url: str, img_dir: str, referer: str = None) -> str:
        """ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°"""
        try:
            if url in self.img_cache:
                return self.img_cache[url]
            
            headers = self.headers.copy()
            if referer:
                headers['Referer'] = referer
            
            response = self.session.get(url, headers=headers, timeout=10)
            if response.status_code == 200:
                # è·å–æ‰©å±•å
                content_type = response.headers.get('content-type', '')
                ext_map = {
                    'image/png': '.png',
                    'image/jpeg': '.jpg',
                    'image/jpg': '.jpg',
                    'image/gif': '.gif',
                    'image/webp': '.webp',
                    'image/svg+xml': '.svg'
                }
                ext = '.png'
                for mime, extension in ext_map.items():
                    if mime in content_type:
                        ext = extension
                        break
                
                img_name = f"img_{abs(hash(url))}{ext}"
                img_path = os.path.join(img_dir, img_name)
                
                with open(img_path, 'wb') as f:
                    f.write(response.content)
                
                result = f"images/{img_name}"
                self.img_cache[url] = result
                return result
                
        except Exception as e:
            print(f"å›¾ç‰‡ä¸‹è½½å¤±è´¥: {url}, {e}")
        
        return url
    
    def clean_html(self, soup):
        """æ¸…ç†HTML - ç§»é™¤æ— å…³å…ƒç´ """
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for tag in soup.find_all(['script', 'style', 'iframe', 'noscript']):
            tag.decompose()
        
        # ç§»é™¤å¹¿å‘Šå…ƒç´ 
        ad_patterns = [
            'ad', 'advertisement', 'adsbygoogle', 'sponsor', 
            'promo', 'banner', 'popup', 'modal'
        ]
        for pattern in ad_patterns:
            for tag in soup.find_all(class_=re.compile(pattern, re.I)):
                tag.decompose()
        
        return soup
    
    def clean_math_formula(self, text: str) -> str:
        """
        æ·±åº¦æ¸…ç†æ•°å­¦å…¬å¼ - ç§»é™¤CSDNçš„XMLæ ‡ç­¾
        å°†å¤æ‚çš„MathMLæ ‡ç­¾è½¬æ¢ä¸ºçº¯LaTeX
        """
        if not text:
            return text
        
        # ç§»é™¤æ‰€æœ‰MathML XMLæ ‡ç­¾,ä¿ç•™çº¯æ–‡æœ¬å…¬å¼
        # å¤„ç†: <semantics><mrow>...</mrow><annotation encoding="application/x-tex">LATEX_HERE</annotation></semantics>
        
        # æå–annotationæ ‡ç­¾ä¸­çš„LaTeX
        annotation_pattern = r'<annotation[^>]*encoding="application/x-tex"[^>]*>(.*?)</annotation>'
        annotations = re.findall(annotation_pattern, text, re.DOTALL)
        
        if annotations:
            # å¦‚æœæ‰¾åˆ°annotation,ç›´æ¥ä½¿ç”¨å…¶ä¸­çš„LaTeX
            return annotations[0].strip()
        
        # ç§»é™¤æ‰€æœ‰XMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        
        # æ¸…ç†å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        return text
    
    def process_math_formulas(self, content):
        """
        å¢å¼ºæ•°å­¦å…¬å¼å¤„ç† - CSDNæ·±åº¦ä¼˜åŒ–
        æ”¯æŒæ ¼å¼:
        - LaTeXè¡Œå†…: \\( ... \\) æˆ– $ ... $
        - LaTeXå—çº§: \\[ ... \\] æˆ– $$ ... $$
        - MathJax scriptæ ‡ç­¾
        - CSDNçš„katex/mathjax spanæ ‡ç­¾
        """
        soup = BeautifulSoup(str(content), 'html.parser')
        
        # å¤„ç†MathJax scriptæ ‡ç­¾
        for script in soup.find_all('script', type='math/tex'):
            formula = script.string
            if formula:
                formula_clean = self.clean_math_formula(formula)
                span = soup.new_tag('span', attrs={'class': 'math-inline'})
                span.string = f'${formula_clean}$'
                script.replace_with(span)
        
        for script in soup.find_all('script', type='math/tex; mode=display'):
            formula = script.string
            if formula:
                formula_clean = self.clean_math_formula(formula)
                div = soup.new_tag('div', attrs={'class': 'math-display'})
                div.string = f'$${formula_clean}$$'
                script.replace_with(div)
        
        # å¤„ç†CSDNçš„katex/mathjax spanæ ‡ç­¾ - æ·±åº¦æ¸…ç†
        for span in soup.find_all('span', class_=re.compile('katex|mathjax|MathJax')):
            # è·å–åŸå§‹HTMLå†…å®¹
            formula_html = str(span)
            formula_text = span.get_text()
            
            # å°è¯•æå–annotationä¸­çš„LaTeX
            formula_clean = self.clean_math_formula(formula_html)
            
            # å¦‚æœæ¸…ç†åä¸ºç©º,ä½¿ç”¨æ–‡æœ¬å†…å®¹
            if not formula_clean or formula_clean == formula_text:
                formula_clean = formula_text
            
            # åˆ¤æ–­æ˜¯è¡Œå†…è¿˜æ˜¯å—çº§
            if 'display' in span.get('class', []) or 'block' in str(span.get('style', '')):
                new_div = soup.new_tag('div', attrs={'class': 'math-display'})
                new_div.string = f'$${formula_clean}$$'
                span.replace_with(new_div)
            else:
                new_span = soup.new_tag('span', attrs={'class': 'math-inline'})
                new_span.string = f'${formula_clean}$'
                span.replace_with(new_span)
        
        # LaTeXè¡Œå†…å…¬å¼: \( ... \)
        content_str = str(soup)
        content_str = re.sub(
            r'\\\((.*?)\\\)', 
            r'<span class="math-inline">$\1$</span>', 
            content_str
        )
        
        # LaTeXå—çº§å…¬å¼: \[ ... \]
        content_str = re.sub(
            r'\\\[(.*?)\\\]', 
            r'<div class="math-display">$$\1$$</div>', 
            content_str, 
            flags=re.DOTALL
        )
        
        return BeautifulSoup(content_str, 'html.parser')
    
    def html_to_markdown(self, content) -> str:
        """
        å¢å¼ºçš„HTMLè½¬Markdown - æ­£ç¡®å¤„ç†æ•°å­¦å…¬å¼
        """
        lines = []
        
        for element in content.descendants:
            # è·³è¿‡NavigableString
            if isinstance(element, NavigableString):
                continue
            
            tag = element.name
            
            # æ•°å­¦å…¬å¼ - ç›´æ¥è½¬æ¢ä¸ºMarkdownè¯­æ³•
            if tag == 'span' and 'math-inline' in element.get('class', []):
                formula = element.get_text().strip()
                if formula.startswith('$') and formula.endswith('$'):
                    lines.append(formula)
                else:
                    lines.append(f'${formula}$')
                continue
            
            if tag == 'div' and 'math-display' in element.get('class', []):
                formula = element.get_text().strip()
                if formula.startswith('$$') and formula.endswith('$$'):
                    lines.append(f'\n{formula}\n')
                else:
                    lines.append(f'\n$${formula}$$\n')
                continue
            
            # æ ‡é¢˜
            if tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                level = int(tag[1])
                text = element.get_text().strip()
                if text:
                    lines.append(f"\n{'#' * level} {text}\n")
                continue
            
            # æ®µè½
            if tag == 'p':
                text = element.get_text().strip()
                if text:
                    lines.append(f"\n{text}\n")
                continue
            
            # ä»£ç å—
            if tag == 'pre':
                code_tag = element.find('code')
                lang = ''
                if code_tag:
                    lang_classes = code_tag.get('class', [])
                    for cls in lang_classes:
                        if cls.startswith('language-'):
                            lang = cls.replace('language-', '')
                            break
                code_text = element.get_text()
                lines.append(f"\n```{lang}\n{code_text}\n```\n")
                continue
            
            # å¼•ç”¨
            if tag == 'blockquote':
                text = element.get_text().strip()
                if text:
                    lines.append(f"\n> {text}\n")
                continue
        
        return ''.join(lines)


# ======================== èœé¸Ÿæ•™ç¨‹çˆ¬è™« ========================

class RunoobCrawler(BaseCrawler):
    """èœé¸Ÿæ•™ç¨‹çˆ¬è™«"""
    
    def extract_tutorial_info(self, url: str) -> Tuple[str, List[Article]]:
        response = self.session.get(url, headers=self.headers, timeout=15)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ ‡é¢˜
        title_tag = soup.find('h1') or soup.find('title')
        title = title_tag.get_text().strip() if title_tag else "æœªçŸ¥æ•™ç¨‹"
        title = re.sub(r'\s*[-|]?\s*èœé¸Ÿæ•™ç¨‹.*', '', title)
        
        # æå–ç« èŠ‚
        articles = []
        sidebar = soup.find('div', {'id': 'leftcolumn'})
        
        if sidebar:
            links = sidebar.find_all('a')
            for link in links:
                href = link.get('href')
                if href and not href.startswith('#') and not href.startswith('javascript'):
                    full_url = urljoin(url, href)
                    link_title = link.get_text().strip()
                    if link_title and len(link_title) > 1:
                        articles.append(Article(link_title, full_url))
        
        # å»é‡
        seen = set()
        unique = []
        for art in articles:
            if art.url not in seen:
                seen.add(art.url)
                unique.append(art)
        
        return title, unique
    
    def extract_article_content(self, article: Article, download_images: bool, img_dir: str):
        try:
            response = self.session.get(article.url, headers=self.headers, timeout=15)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            content = soup.find('div', {'id': 'content'}) or soup.find('article') or soup.find('div', class_='article-intro')
            
            if not content:
                article.content = "_å†…å®¹è·å–å¤±è´¥_"
                article.html_content = "<p><em>å†…å®¹è·å–å¤±è´¥</em></p>"
                return
            
            content = self.clean_html(content)
            content = self.process_math_formulas(content)
            
            # å¤„ç†å›¾ç‰‡
            if download_images:
                for img in content.find_all('img'):
                    src = img.get('src') or img.get('data-src')
                    if src:
                        img_url = urljoin(article.url, src)
                        local_path = self.download_image(img_url, img_dir)
                        img['src'] = local_path
                        if not img.get('alt'):
                            img['alt'] = 'image'
            
            # å¤„ç†ä»£ç å—
            for pre in content.find_all('pre'):
                pre['class'] = 'code-block'
            
            article.html_content = str(content)
            article.content = self.html_to_markdown(content)
            
        except Exception as e:
            article.content = f"_æå–å¤±è´¥: {str(e)}_"
            article.html_content = f"<p><em>æå–å¤±è´¥: {str(e)}</em></p>"


# ======================== CSDNçˆ¬è™« - æ•°å­¦å…¬å¼æ·±åº¦ä¼˜åŒ–ç‰ˆ ========================

class CSDNCrawler(BaseCrawler):
    """CSDNåšå®¢çˆ¬è™« - æ•°å­¦å…¬å¼æ·±åº¦ä¼˜åŒ–"""
    
    def __init__(self):
        super().__init__()
        self.headers.update({
            'Referer': 'https://blog.csdn.net/',
            'Cookie': 'uuid_tt_dd=10_12345678-1234567890123-0123456789012-0123456789012'
        })
    
    def extract_column_articles(self, url: str) -> List[Article]:
        """æå–ä¸“æ æ–‡ç« åˆ—è¡¨"""
        articles = []
        
        try:
            response = self.session.get(url, headers=self.headers, timeout=15)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å¤šç§æ–¹å¼æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨
            selectors = [
                ('div', {'class': 'column_article_list'}),
                ('div', {'class': re.compile('article.*item')}),
                ('div', {'class': re.compile('blog.*item')}),
                ('article', {}),
            ]
            
            article_items = []
            for tag, attrs in selectors:
                items = soup.find_all(tag, attrs)
                if items:
                    article_items.extend(items)
                    break
            
            # æå–æ–‡ç« é“¾æ¥
            for item in article_items:
                link = item.find('a', href=re.compile('/article/details/'))
                if link:
                    title = link.get_text().strip()
                    href = link.get('href')
                    if not href.startswith('http'):
                        href = urljoin('https://blog.csdn.net', href)
                    articles.append(Article(title, href))
            
            # å¦‚æœä¸Šè¿°æ–¹æ³•éƒ½å¤±è´¥ï¼Œç›´æ¥æŸ¥æ‰¾æ‰€æœ‰æ–‡ç« é“¾æ¥
            if not articles:
                links = soup.find_all('a', href=re.compile('/article/details/'))
                for link in links:
                    title = link.get_text().strip()
                    if title and len(title) > 3:
                        href = link.get('href')
                        if not href.startswith('http'):
                            href = urljoin('https://blog.csdn.net', href)
                        articles.append(Article(title, href))
            
        except Exception as e:
            print(f"æå–ä¸“æ æ–‡ç« å¤±è´¥: {e}")
        
        # å»é‡
        seen = set()
        unique = []
        for art in articles:
            if art.url not in seen and '/article/details/' in art.url:
                seen.add(art.url)
                unique.append(art)
        
        return unique
    
    def extract_article_info(self, article: Article):
        """æå–æ–‡ç« å…ƒä¿¡æ¯"""
        try:
            response = self.session.get(article.url, headers=self.headers, timeout=15)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–æ ‡é¢˜
            if not article.title or article.title == "æœªå‘½å":
                title_selectors = [
                    ('h1', {'class': 'title-article'}),
                    ('h1', {'id': 'articleContentId'}),
                    ('h1', {}),
                ]
                for tag, attrs in title_selectors:
                    title_tag = soup.find(tag, attrs)
                    if title_tag:
                        article.title = title_tag.get_text().strip()
                        break
            
            # æå–ä½œè€…
            author_selectors = [
                ('a', {'class': 'follow-nickName'}),
                ('a', {'class': re.compile('user.*name')}),
                ('div', {'class': 'user-info'}),
            ]
            for tag, attrs in author_selectors:
                author_tag = soup.find(tag, attrs)
                if author_tag:
                    article.author = author_tag.get_text().strip()
                    break
            
            # æå–æ—¥æœŸ
            date_selectors = [
                ('span', {'class': 'time'}),
                ('span', {'class': re.compile('date|time')}),
            ]
            for tag, attrs in date_selectors:
                date_tag = soup.find(tag, attrs)
                if date_tag:
                    article.date = date_tag.get_text().strip()
                    break
            
            # æå–åˆ†ç±»
            category_tag = soup.find('a', {'class': 'tag-link'})
            if category_tag:
                article.category = category_tag.get_text().strip()
            
        except Exception as e:
            print(f"æå–æ–‡ç« ä¿¡æ¯å¤±è´¥: {e}")
    
    def extract_article_content(self, article: Article, download_images: bool, img_dir: str):
        """å¢å¼ºçš„å†…å®¹æå– - æ•°å­¦å…¬å¼æ·±åº¦ä¼˜åŒ–"""
        try:
            self.extract_article_info(article)
            
            response = self.session.get(article.url, headers=self.headers, timeout=15)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å¤šç§æ–¹å¼æŸ¥æ‰¾å†…å®¹åŒºåŸŸ
            content_selectors = [
                ('div', {'id': 'content_views'}),
                ('div', {'class': 'article_content'}),
                ('div', {'class': re.compile('article.*content')}),
                ('article', {}),
            ]
            
            content = None
            for tag, attrs in content_selectors:
                content = soup.find(tag, attrs)
                if content:
                    break
            
            if not content:
                article.content = "_å†…å®¹è·å–å¤±è´¥_"
                article.html_content = "<p><em>å†…å®¹è·å–å¤±è´¥</em></p>"
                return
            
            # æ¸…ç†HTML
            content = self.clean_html(content)
            
            # ç§»é™¤CSDNç‰¹æœ‰çš„å¹²æ‰°å…ƒç´ 
            csdn_noise_patterns = [
                'hljs-button', 'csdn-tracking', 'hide-article',
                'blog-content-box', 'recommend-box', 'comment-box',
                'tool-box', 'more-toolbox', 'opt-box'
            ]
            for pattern in csdn_noise_patterns:
                for tag in content.find_all(class_=re.compile(pattern, re.I)):
                    tag.decompose()
            
            # ä¿®å¤: ä½¿ç”¨ string å‚æ•°æ›¿ä»£ text å‚æ•°
            for tag in content.find_all(string=re.compile('å·²æ”¶å½•|ç‰ˆæƒå£°æ˜|Â©ï¸|æŸ¥çœ‹åŸæ–‡')):
                parent = tag.parent
                if parent:
                    parent.decompose()
            
            # å¤„ç†å›¾ç‰‡ - CSDNéœ€è¦ç‰¹æ®Šå¤„ç†
            if download_images:
                for img in content.find_all('img'):
                    # CSDNå›¾ç‰‡å¯èƒ½åœ¨å¤šä¸ªå±æ€§ä¸­
                    src = img.get('src') or img.get('data-src') or img.get('data-original-src')
                    if src:
                        img_url = urljoin(article.url, src)
                        local_path = self.download_image(img_url, img_dir, article.url)
                        img['src'] = local_path
                        if not img.get('alt'):
                            img['alt'] = 'image'
            
            # å¤„ç†æ•°å­¦å…¬å¼ - æ·±åº¦ä¼˜åŒ–
            content = self.process_math_formulas(content)
            
            # å¤„ç†ä»£ç å—
            for pre in content.find_all('pre'):
                # ä¿ç•™ä»£ç å—çš„è¯­è¨€æ ‡è¯†
                code_tag = pre.find('code')
                if code_tag:
                    lang_classes = code_tag.get('class', [])
                    for cls in lang_classes:
                        if cls.startswith('language-'):
                            lang = cls.replace('language-', '')
                            pre['data-lang'] = lang
                            break
                pre['class'] = 'code-block'
            
            article.html_content = str(content)
            article.content = self.html_to_markdown(content)
            
        except Exception as e:
            article.content = f"_æå–å¤±è´¥: {str(e)}_"
            article.html_content = f"<p><em>æå–å¤±è´¥: {str(e)}</em></p>"


# ======================== GitBookçˆ¬è™« ========================

class GitBookCrawler(BaseCrawler):
    """GitBookæ–‡æ¡£çˆ¬è™«"""
    
    def extract_gitbook_info(self, url: str) -> Tuple[str, List[Article]]:
        """æå–GitBookçš„æ–‡æ¡£ç»“æ„"""
        response = self.session.get(url, headers=self.headers, timeout=15)
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æå–æ ‡é¢˜
        title_tag = soup.find('h1') or soup.find('title')
        title = title_tag.get_text().strip() if title_tag else "GitBookæ–‡æ¡£"
        
        # æå–ç›®å½•/ç« èŠ‚
        articles = []
        
        # GitBooké€šå¸¸æœ‰å¯¼èˆªæ æˆ–ç›®å½•
        nav_selectors = [
            ('nav', {'class': re.compile('book-summary|navigation|sidebar')}),
            ('div', {'class': re.compile('toc|summary|navigation')}),
            ('aside', {}),
        ]
        
        nav = None
        for tag, attrs in nav_selectors:
            nav = soup.find(tag, attrs)
            if nav:
                break
        
        if nav:
            # æå–æ‰€æœ‰é“¾æ¥
            links = nav.find_all('a', href=True)
            for link in links:
                href = link.get('href')
                if href and not href.startswith('#') and not href.startswith('javascript'):
                    # è·³è¿‡å¤–éƒ¨é“¾æ¥
                    if href.startswith('http') and urlparse(href).netloc != urlparse(url).netloc:
                        continue
                    
                    full_url = urljoin(url, href)
                    link_title = link.get_text().strip()
                    if link_title and len(link_title) > 1:
                        articles.append(Article(link_title, full_url))
        
        # å¦‚æœæ²¡æ‰¾åˆ°å¯¼èˆªï¼Œè‡³å°‘æ·»åŠ å½“å‰é¡µ
        if not articles:
            articles.append(Article(title, url))
        
        # å»é‡
        seen = set()
        unique = []
        for art in articles:
            if art.url not in seen:
                seen.add(art.url)
                unique.append(art)
        
        return title, unique
    
    def extract_article_content(self, article: Article, download_images: bool, img_dir: str):
        """æå–GitBookæ–‡ç« å†…å®¹"""
        try:
            response = self.session.get(article.url, headers=self.headers, timeout=15)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # GitBookå†…å®¹åŒºåŸŸé€‰æ‹©å™¨
            content_selectors = [
                ('div', {'class': re.compile('page-wrapper|markdown-section|book-body')}),
                ('article', {}),
                ('main', {}),
                ('div', {'class': 'content'}),
            ]
            
            content = None
            for tag, attrs in content_selectors:
                content = soup.find(tag, attrs)
                if content:
                    break
            
            if not content:
                article.content = "_å†…å®¹è·å–å¤±è´¥_"
                article.html_content = "<p><em>å†…å®¹è·å–å¤±è´¥</em></p>"
                return
            
            # æ¸…ç†HTML
            content = self.clean_html(content)
            
            # ç§»é™¤å¯¼èˆªå…ƒç´ 
            for tag in content.find_all(class_=re.compile('navigation|sidebar|toc-menu')):
                tag.decompose()
            
            # å¤„ç†æ•°å­¦å…¬å¼
            content = self.process_math_formulas(content)
            
            # å¤„ç†å›¾ç‰‡
            if download_images:
                for img in content.find_all('img'):
                    src = img.get('src') or img.get('data-src')
                    if src:
                        img_url = urljoin(article.url, src)
                        local_path = self.download_image(img_url, img_dir, article.url)
                        img['src'] = local_path
                        if not img.get('alt'):
                            img['alt'] = 'image'
            
            # å¤„ç†ä»£ç å—
            for pre in content.find_all('pre'):
                code_tag = pre.find('code')
                if code_tag:
                    lang_classes = code_tag.get('class', [])
                    for cls in lang_classes:
                        if cls.startswith('language-'):
                            lang = cls.replace('language-', '')
                            pre['data-lang'] = lang
                            break
                pre['class'] = 'code-block'
            
            article.html_content = str(content)
            article.content = self.html_to_markdown(content)
            
        except Exception as e:
            article.content = f"_æå–å¤±è´¥: {str(e)}_"
            article.html_content = f"<p><em>æå–å¤±è´¥: {str(e)}</em></p>"


# ======================== çˆ¬è™«çº¿ç¨‹ ========================

class CrawlerThread(QThread):
    """çˆ¬è™«çº¿ç¨‹ - å¢å¼ºç‰ˆ"""
    progress_signal = pyqtSignal(str)
    finished_signal = pyqtSignal(bool, str)
    
    def __init__(self):
        super().__init__()
        self.url = ""
        self.platform = "runoob"
        self.output_dir = "./output"
        self.output_formats = ['markdown', 'html', 'pdf']
        self.download_images = True
        self.aggregate_mode = True  # True=åˆå¹¶æˆä¸€ä¸ªæ–‡ä»¶ï¼ŒFalse=æ¯ç¯‡ç‹¬ç«‹æ–‡ä»¶
        self.is_running = True
        
    def run(self):
        try:
            Path(self.output_dir).mkdir(parents=True, exist_ok=True)
            img_dir = os.path.join(self.output_dir, 'images')
            Path(img_dir).mkdir(parents=True, exist_ok=True)
            
            if self.platform == 'runoob':
                self.crawl_runoob()
            elif self.platform == 'csdn':
                self.crawl_csdn()
            elif self.platform == 'gitbook':
                self.crawl_gitbook()
            
            self.finished_signal.emit(True, f"âœ… å®Œæˆ!\nä¿å­˜ä½ç½®: {os.path.abspath(self.output_dir)}")
            
        except Exception as e:
            import traceback
            error_detail = traceback.format_exc()
            self.finished_signal.emit(False, f"âŒ é”™è¯¯: {str(e)}\n\n{error_detail}")
    
    def stop(self):
        self.is_running = False
    
    def crawl_runoob(self):
        self.progress_signal.emit("ğŸ“– æ­£åœ¨åˆ†æèœé¸Ÿæ•™ç¨‹...")
        
        crawler = RunoobCrawler()
        title, articles = crawler.extract_tutorial_info(self.url)
        
        if not articles:
            raise Exception("æœªæ‰¾åˆ°ä»»ä½•ç« èŠ‚")
        
        self.progress_signal.emit(f"ğŸ“š æ•™ç¨‹: {title}")
        self.progress_signal.emit(f"ğŸ“‘ å…± {len(articles)} ä¸ªç« èŠ‚")
        
        img_dir = os.path.join(self.output_dir, 'images')
        
        for idx, article in enumerate(articles, 1):
            if not self.is_running:
                return
            
            self.progress_signal.emit(f"ğŸ“„ [{idx}/{len(articles)}] {article.title}")
            crawler.extract_article_content(article, self.download_images, img_dir)
            time.sleep(0.5)
        
        # æ ¹æ®æ¨¡å¼ç”Ÿæˆæ–‡ä»¶
        if self.aggregate_mode:
            self.generate_files(title, articles, "èœé¸Ÿæ•™ç¨‹")
        else:
            self.generate_separate_files(articles)
    
    def crawl_csdn(self):
        self.progress_signal.emit("ğŸ“– æ­£åœ¨åˆ†æCSDN...")
        
        crawler = CSDNCrawler()
        
        # åˆ¤æ–­æ˜¯ä¸“æ è¿˜æ˜¯å•ç¯‡æ–‡ç« 
        if '/column/info/' in self.url or '/category_' in self.url:
            # ä¸“æ æ¨¡å¼
            articles = crawler.extract_column_articles(self.url)
            if not articles:
                raise Exception("æœªæ‰¾åˆ°ä»»ä½•æ–‡ç« ")
            
            title = f"CSDNä¸“æ _{len(articles)}ç¯‡"
            self.progress_signal.emit(f"ğŸ“š ä¸“æ : {len(articles)}ç¯‡æ–‡ç« ")
        else:
            # å•ç¯‡æ–‡ç« æ¨¡å¼
            articles = [Article("æœªå‘½å", self.url)]
            title = "CSDNæ–‡ç« "
            self.progress_signal.emit(f"ğŸ“„ å•ç¯‡æ–‡ç« ")
        
        img_dir = os.path.join(self.output_dir, 'images')
        
        for idx, article in enumerate(articles, 1):
            if not self.is_running:
                return
            
            self.progress_signal.emit(f"ğŸ“„ [{idx}/{len(articles)}] æå–ä¸­...")
            crawler.extract_article_content(article, self.download_images, img_dir)
            time.sleep(1)  # CSDNéœ€è¦æ›´é•¿å»¶è¿Ÿ
        
        # æ ¹æ®æ¨¡å¼ç”Ÿæˆæ–‡ä»¶
        if self.aggregate_mode and len(articles) > 1:
            self.generate_files(title, articles, articles[0].author if articles else "")
        else:
            self.generate_separate_files(articles)
    
    def crawl_gitbook(self):
        self.progress_signal.emit("ğŸ“– æ­£åœ¨åˆ†æGitBook...")
        
        crawler = GitBookCrawler()
        title, articles = crawler.extract_gitbook_info(self.url)
        
        if not articles:
            raise Exception("æœªæ‰¾åˆ°ä»»ä½•ç« èŠ‚")
        
        self.progress_signal.emit(f"ğŸ“š æ–‡æ¡£: {title}")
        self.progress_signal.emit(f"ğŸ“‘ å…± {len(articles)} ä¸ªç« èŠ‚")
        
        img_dir = os.path.join(self.output_dir, 'images')
        
        for idx, article in enumerate(articles, 1):
            if not self.is_running:
                return
            
            self.progress_signal.emit(f"ğŸ“„ [{idx}/{len(articles)}] {article.title}")
            crawler.extract_article_content(article, self.download_images, img_dir)
            time.sleep(0.5)
        
        # æ ¹æ®æ¨¡å¼ç”Ÿæˆæ–‡ä»¶
        if self.aggregate_mode:
            self.generate_files(title, articles, "GitBook")
        else:
            self.generate_separate_files(articles)
    
    def generate_separate_files(self, articles: List[Article]):
        """æ¯ç¯‡æ–‡ç« ç‹¬ç«‹æ–‡ä»¶ - ä½¿ç”¨æ–‡ç« æ ‡é¢˜å‘½å"""
        for idx, article in enumerate(articles, 1):
            if not self.is_running:
                return
            
            # ä½¿ç”¨æ–‡ç« æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
            safe_title = re.sub(r'[\\/:"*?<>|]+', '_', article.title)
            safe_title = safe_title.strip()[:100]  # é™åˆ¶é•¿åº¦
            
            if not safe_title or safe_title == "æœªå‘½å":
                safe_title = f"æ–‡ç« _{idx}"
            
            if 'markdown' in self.output_formats:
                md_path = os.path.join(self.output_dir, f"{safe_title}.md")
                self.generate_markdown(md_path, article.title, [article], article.author)
            
            if 'html' in self.output_formats:
                html_path = os.path.join(self.output_dir, f"{safe_title}.html")
                self.generate_html(html_path, article.title, [article], article.author)
            
            if 'pdf' in self.output_formats and WEASY_AVAILABLE:
                pdf_path = os.path.join(self.output_dir, f"{safe_title}.pdf")
                self.generate_pdf(pdf_path, article.title, [article], article.author)
            
            self.progress_signal.emit(f"âœ… [{idx}/{len(articles)}] {safe_title}")
    
    def generate_files(self, title: str, articles: List[Article], author: str):
        """èšåˆæ¨¡å¼ - ä½¿ç”¨æ ‡é¢˜å‘½å"""
        # æ™ºèƒ½æ–‡ä»¶å
        if articles and articles[0].title and articles[0].title != "æœªå‘½å":
            safe_title = re.sub(r'[\\/:"*?<>|]+', '_', articles[0].title)
        else:
            safe_title = re.sub(r'[\\/:"*?<>|]+', '_', title)
        
        safe_title = safe_title.strip()[:100]
        if not safe_title:
            safe_title = "æ–‡æ¡£"
        
        if 'markdown' in self.output_formats:
            md_path = os.path.join(self.output_dir, f"{safe_title}.md")
            self.generate_markdown(md_path, title, articles, author)
        
        if 'html' in self.output_formats:
            html_path = os.path.join(self.output_dir, f"{safe_title}.html")
            self.generate_html(html_path, title, articles, author)
        
        if 'pdf' in self.output_formats and WEASY_AVAILABLE:
            pdf_path = os.path.join(self.output_dir, f"{safe_title}.pdf")
            self.generate_pdf(pdf_path, title, articles, author)
    
    def generate_markdown(self, filepath: str, title: str, articles: List[Article], author: str):
        """ç”ŸæˆMarkdownæ–‡ä»¶ - ä¼˜åŒ–æ•°å­¦å…¬å¼"""
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"# {title}\n\n")
            f.write(f"> **ä½œè€…**: {author}\n")
            f.write(f"> **ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write("---\n\n")
            
            for idx, art in enumerate(articles, 1):
                if len(articles) > 1:
                    f.write(f"## {idx}. {art.title}\n\n")
                else:
                    f.write(f"## {art.title}\n\n")
                
                if art.author:
                    f.write(f"**ä½œè€…**: {art.author}  \n")
                if art.date:
                    f.write(f"**æ—¥æœŸ**: {art.date}  \n")
                if art.url:
                    f.write(f"**åŸæ–‡**: {art.url}  \n")
                
                f.write("\n")
                f.write(art.content)
                f.write("\n\n---\n\n")
        
        self.progress_signal.emit(f"âœ… Markdown: {os.path.basename(filepath)}")
    
    def generate_html(self, filepath: str, title: str, articles: List[Article], platform: str):
        """ç”ŸæˆHTMLæ–‡ä»¶ - å¢å¼ºæ•°å­¦å…¬å¼æ”¯æŒ"""
        html_template = """<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>{title}</title>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, "Noto Sans", sans-serif, "Microsoft YaHei";
            line-height: 1.8;
            color: #333;
            background: #f5f5f5;
            padding: 20px;
        }}
        .container {{
            max-width: 900px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            border-radius: 8px;
        }}
        h1 {{
            font-size: 2.5em;
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 15px;
            margin-bottom: 30px;
        }}
        h2 {{
            font-size: 2em;
            color: #34495e;
            margin-top: 40px;
            margin-bottom: 20px;
            padding-left: 15px;
            border-left: 5px solid #3498db;
        }}
        h3 {{
            font-size: 1.5em;
            color: #555;
            margin-top: 30px;
            margin-bottom: 15px;
        }}
        .meta {{
            background: #ecf0f1;
            padding: 20px;
            border-radius: 5px;
            margin-bottom: 30px;
            font-size: 0.95em;
        }}
        .meta strong {{
            color: #2980b9;
        }}
        .article {{
            margin-bottom: 50px;
            padding-bottom: 30px;
            border-bottom: 2px dashed #ddd;
        }}
        .article:last-child {{
            border-bottom: none;
        }}
        .article-meta {{
            color: #7f8c8d;
            font-size: 0.9em;
            margin-bottom: 15px;
        }}
        p {{
            margin: 15px 0;
            text-align: justify;
        }}
        img {{
            max-width: 100%;
            height: auto;
            display: block;
            margin: 20px auto;
            border-radius: 5px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }}
        .code-block, pre {{
            background: #2d2d2d;
            color: #f8f8f2;
            padding: 20px;
            border-radius: 5px;
            overflow-x: auto;
            margin: 20px 0;
            font-family: 'Consolas', 'Monaco', 'Courier New', monospace;
            font-size: 0.9em;
            line-height: 1.5;
        }}
        blockquote {{
            border-left: 4px solid #3498db;
            padding-left: 20px;
            margin: 20px 0;
            color: #555;
            background: #f9f9f9;
            padding: 15px 20px;
            border-radius: 0 5px 5px 0;
        }}
        /* æ•°å­¦å…¬å¼æ ·å¼ */
        .math-inline {{
            display: inline;
            margin: 0 2px;
        }}
        .math-display {{
            display: block;
            margin: 20px 0;
            text-align: center;
            overflow-x: auto;
            padding: 15px;
            background: #f9f9f9;
            border-radius: 5px;
        }}
        a {{
            color: #3498db;
            text-decoration: none;
        }}
        a:hover {{
            text-decoration: underline;
        }}
        .timestamp {{
            text-align: center;
            color: #95a5a6;
            font-size: 0.85em;
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #ecf0f1;
        }}
    </style>
    <!-- MathJax 3.x é…ç½® - æ”¯æŒæ‰€æœ‰LaTeXå…¬å¼ -->
    <script>
        window.MathJax = {{
            tex: {{
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true,
                tags: 'ams',
                packages: {{'[+]': ['ams', 'newcommand', 'configmacros']}}
            }},
            options: {{
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
                ignoreHtmlClass: 'tex2jax_ignore',
                processHtmlClass: 'tex2jax_process'
            }},
            startup: {{
                pageReady: () => {{
                    return MathJax.startup.defaultPageReady().then(() => {{
                        console.log('MathJax å·²åŠ è½½å®Œæˆ');
                    }});
                }}
            }},
            svg: {{
                fontCache: 'global'
            }}
        }};
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script>
</head>
<body>
    <div class="container">
        <h1>{title}</h1>
        <div class="meta">
            <p><strong>å¹³å°</strong>: {platform}</p>
            <p><strong>ç”Ÿæˆæ—¶é—´</strong>: {generation_time}</p>
            <p><strong>ç« èŠ‚æ•°</strong>: {article_count}</p>
        </div>
        {articles_html}
        <div class="timestamp">
            Generated by Web Content Extractor Pro v8.1
        </div>
    </div>
</body>
</html>"""
        
        articles_html = []
        for idx, article in enumerate(articles, 1):
            article_html = f'<div class="article">'
            article_html += f'<h2>{idx}. {article.title}</h2>'
            
            meta_parts = []
            if article.url:
                meta_parts.append(f'<a href="{article.url}" target="_blank">æŸ¥çœ‹åŸæ–‡</a>')
            if article.author:
                meta_parts.append(f'ä½œè€…: {article.author}')
            if article.date:
                meta_parts.append(f'æ—¥æœŸ: {article.date}')
            
            if meta_parts:
                article_html += f'<div class="article-meta">{" | ".join(meta_parts)}</div>'
            
            article_html += article.html_content
            article_html += '</div>'
            articles_html.append(article_html)
        
        html_content = html_template.format(
            title=title,
            platform=platform,
            generation_time=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            article_count=len(articles),
            articles_html=''.join(articles_html)
        )
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        self.progress_signal.emit(f"âœ… HTML: {os.path.basename(filepath)}")
    
    def generate_pdf(self, filepath: str, title: str, articles: List[Article], platform: str):
        """ç”ŸæˆPDFæ–‡ä»¶ - ä¼˜åŒ–å­—ä½“é…ç½®"""
        if not WEASY_AVAILABLE:
            self.progress_signal.emit("âš ï¸ WeasyPrintæœªå®‰è£…ï¼Œè·³è¿‡PDFç”Ÿæˆ")
            return
        
        try:
            # å…ˆç”Ÿæˆä¸´æ—¶HTML
            import tempfile
            with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False, encoding='utf-8') as tmp:
                tmp_path = tmp.name
                self.generate_html(tmp_path, title, articles, platform)
            
            # åˆ›å»ºå­—ä½“é…ç½® - æŠ‘åˆ¶è­¦å‘Š
            font_config = FontConfiguration()
            
            # ç”ŸæˆPDF
            HTML(filename=tmp_path).write_pdf(
                filepath,
                stylesheets=[CSS(string=self.get_pdf_css())],
                font_config=font_config
            )
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.remove(tmp_path)
            except:
                pass
            
            self.progress_signal.emit(f"âœ… PDF: {os.path.basename(filepath)}")
            
        except Exception as e:
            self.progress_signal.emit(f"âš ï¸ PDFç”Ÿæˆå¤±è´¥: {str(e)}")
    
    def get_pdf_css(self) -> str:
        """PDFä¸“ç”¨CSS - ä¼˜åŒ–æ‰“å°æ•ˆæœ"""
        return """
        @page {
            size: A4;
            margin: 2.5cm 2cm;
        }
        body {
            font-family: "Microsoft YaHei", "SimSun", sans-serif;
            font-size: 11pt;
            line-height: 1.7;
        }
        .chapter {
            page-break-before: always;
        }
        h1, h2, h3 {
            page-break-after: avoid;
        }
        h1 { font-size: 24pt; }
        h2 { font-size: 18pt; margin-top: 20pt; }
        h3 { font-size: 14pt; }
        .article {
            page-break-after: always;
        }
        .code-block, pre {
            font-size: 9pt;
            page-break-inside: avoid;
            background: #f5f5f5;
            border: 1px solid #ddd;
        }
        img {
            max-width: 100%;
            page-break-inside: avoid;
        }
        .math-inline {
            font-family: "Times New Roman", "STIX Two Math", serif;
        }
        .math-display {
            text-align: center;
            margin: 20px 0;
            font-family: "Times New Roman", serif;
        }
        """


# ======================== GUIä¸»çª—å£ ========================

class MainWindow(QMainWindow):
    """ä¸»çª—å£ - ç°ä»£åŒ–UIè®¾è®¡"""
    
    def __init__(self):
        super().__init__()
        self.crawler_thread = None
        self.settings = QSettings('WebExtractor', 'v8.1')
        self.init_ui()
        self.load_settings()
    
    def init_ui(self):
        self.setWindowTitle('Web Content Extractor Pro v8.1 - æ•°å­¦å…¬å¼ä¼˜åŒ–ç‰ˆ')
        self.setGeometry(100, 100, 1000, 700)
        
        # è®¾ç½®åº”ç”¨å­—ä½“
        app_font = QFont('Microsoft YaHei', 10)
        self.setFont(app_font)
        
        # ä¸­å¤®éƒ¨ä»¶
        central_widget = QWidget()
        self.setCentralWidget(central_widget)
        main_layout = QVBoxLayout(central_widget)
        main_layout.setSpacing(15)
        main_layout.setContentsMargins(20, 20, 20, 20)
        
        # ===== æ ‡é¢˜åŒºåŸŸ =====
        title_label = QLabel('ğŸ“š Web Content Extractor Pro')
        title_font = QFont('Microsoft YaHei', 18, QFont.Bold)
        title_label.setFont(title_font)
        title_label.setStyleSheet('color: #2c3e50; padding: 10px;')
        main_layout.addWidget(title_label)
        
        # ===== URLè¾“å…¥åŒºåŸŸ =====
        url_group = QGroupBox('ğŸ“ URLåœ°å€')
        url_group.setFont(QFont('Microsoft YaHei', 11, QFont.Bold))
        url_layout = QVBoxLayout()
        
        self.url_input = QLineEdit()
        self.url_input.setPlaceholderText('è¯·è¾“å…¥ç½‘é¡µURL...')
        self.url_input.setFont(QFont('Microsoft YaHei', 10))
        self.url_input.setMinimumHeight(40)
        url_layout.addWidget(self.url_input)
        
        url_group.setLayout(url_layout)
        main_layout.addWidget(url_group)
        
        # ===== å¹³å°é€‰æ‹© =====
        platform_group = QGroupBox('ğŸŒ é€‰æ‹©å¹³å°')
        platform_group.setFont(QFont('Microsoft YaHei', 11, QFont.Bold))
        platform_layout = QHBoxLayout()
        
        self.platform_combo = QComboBox()
        self.platform_combo.addItems([
            'èœé¸Ÿæ•™ç¨‹ (runoob.com)',
            'CSDNåšå®¢/ä¸“æ  (blog.csdn.net)',
            'GitBookæ–‡æ¡£ (*.gitbook.io)'
        ])
        self.platform_combo.setFont(QFont('Microsoft YaHei', 10))
        self.platform_combo.setMinimumHeight(35)
        platform_layout.addWidget(self.platform_combo)
        
        platform_group.setLayout(platform_layout)
        main_layout.addWidget(platform_group)
        
        # ===== è¾“å‡ºé€‰é¡¹ =====
        options_group = QGroupBox('âš™ï¸ è¾“å‡ºé€‰é¡¹')
        options_group.setFont(QFont('Microsoft YaHei', 11, QFont.Bold))
        options_layout = QVBoxLayout()
        
        # è¾“å‡ºæ ¼å¼
        format_layout = QHBoxLayout()
        format_label = QLabel('è¾“å‡ºæ ¼å¼:')
        format_label.setFont(QFont('Microsoft YaHei', 10))
        format_layout.addWidget(format_label)
        
        self.markdown_check = QCheckBox('Markdown')
        self.html_check = QCheckBox('HTML')
        self.pdf_check = QCheckBox('PDF')
        for cb in [self.markdown_check, self.html_check, self.pdf_check]:
            cb.setFont(QFont('Microsoft YaHei', 10))
            cb.setChecked(True)
            format_layout.addWidget(cb)
        
        format_layout.addStretch()
        options_layout.addLayout(format_layout)
        
        # å…¶ä»–é€‰é¡¹
        self.download_img_check = QCheckBox('ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°')
        self.download_img_check.setChecked(True)
        self.download_img_check.setFont(QFont('Microsoft YaHei', 10))
        options_layout.addWidget(self.download_img_check)
        
        self.aggregate_check = QCheckBox('åˆå¹¶ä¸ºå•ä¸ªæ–‡ä»¶ï¼ˆå–æ¶ˆåˆ™æ¯ç« ç‹¬ç«‹ï¼‰')
        self.aggregate_check.setChecked(True)
        self.aggregate_check.setFont(QFont('Microsoft YaHei', 10))
        options_layout.addWidget(self.aggregate_check)
        
        # è¾“å‡ºç›®å½•
        dir_layout = QHBoxLayout()
        dir_label = QLabel('è¾“å‡ºç›®å½•:')
        dir_label.setFont(QFont('Microsoft YaHei', 10))
        dir_layout.addWidget(dir_label)
        
        self.output_dir_input = QLineEdit('./output')
        self.output_dir_input.setFont(QFont('Microsoft YaHei', 10))
        dir_layout.addWidget(self.output_dir_input)
        
        dir_btn = QPushButton('æµè§ˆ...')
        dir_btn.setFont(QFont('Microsoft YaHei', 10))
        dir_btn.clicked.connect(self.select_output_dir)
        dir_layout.addWidget(dir_btn)
        
        options_layout.addLayout(dir_layout)
        options_group.setLayout(options_layout)
        main_layout.addWidget(options_group)
        
        # ===== æ§åˆ¶æŒ‰é’® =====
        button_layout = QHBoxLayout()
        
        self.start_btn = QPushButton('ğŸš€ å¼€å§‹æå–')
        self.start_btn.setFont(QFont('Microsoft YaHei', 12, QFont.Bold))
        self.start_btn.setMinimumHeight(45)
        self.start_btn.setStyleSheet("""
            QPushButton {
                background-color: #3498db;
                color: white;
                border: none;
                border-radius: 5px;
                padding: 10px;
            }
            QPushButton:hover {
                background-color: #2980b9;
            }
            QPushButton:pressed {
                background-color: #21618c;
            }
            QPushButton:disabled {
                background-color: #bdc3c7;
            }
        """)
        self.start_btn.clicked.connect(self.start_crawl)
        button_layout.addWidget(self.start_btn)
        
        self.stop_btn = QPushButton('â¹ï¸ åœæ­¢')
        self.stop_btn.setFont(QFont('Microsoft YaHei', 12, QFont.Bold))
        self.stop_btn.setMinimumHeight(45)
        self.stop_btn.setEnabled(False)
        self.stop_btn.setStyleSheet("""
            QPushButton {
                background-color: #e74c3c;
                color: white;
                border: none;
                border-radius: 5px;
                padding: 10px;
            }
            QPushButton:hover {
                background-color: #c0392b;
            }
            QPushButton:disabled {
                background-color: #bdc3c7;
            }
        """)
        self.stop_btn.clicked.connect(self.stop_crawl)
        button_layout.addWidget(self.stop_btn)
        
        main_layout.addLayout(button_layout)
        
        # ===== æ—¥å¿—åŒºåŸŸ =====
        log_group = QGroupBox('ğŸ“‹ è¿è¡Œæ—¥å¿—')
        log_group.setFont(QFont('Microsoft YaHei', 11, QFont.Bold))
        log_layout = QVBoxLayout()
        
        self.log_text = QTextEdit()
        self.log_text.setReadOnly(True)
        self.log_text.setFont(QFont('Consolas', 9))
        self.log_text.setMinimumHeight(200)
        log_layout.addWidget(self.log_text)
        
        clear_btn = QPushButton('æ¸…ç©ºæ—¥å¿—')
        clear_btn.setFont(QFont('Microsoft YaHei', 9))
        clear_btn.clicked.connect(self.log_text.clear)
        log_layout.addWidget(clear_btn)
        
        log_group.setLayout(log_layout)
        main_layout.addWidget(log_group)
        
        # çŠ¶æ€æ 
        self.statusBar().showMessage('å°±ç»ª')
        self.statusBar().setFont(QFont('Microsoft YaHei', 9))
    
    def select_output_dir(self):
        dir_path = QFileDialog.getExistingDirectory(self, 'é€‰æ‹©è¾“å‡ºç›®å½•')
        if dir_path:
            self.output_dir_input.setText(dir_path)
    
    def log_message(self, message: str):
        self.log_text.append(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")
        self.log_text.moveCursor(QTextCursor.End)
    
    def start_crawl(self):
        url = self.url_input.text().strip()
        if not url:
            QMessageBox.warning(self, 'è­¦å‘Š', 'è¯·è¾“å…¥URLåœ°å€!')
            return
        
        # æ£€æŸ¥è¾“å‡ºæ ¼å¼
        output_formats = []
        if self.markdown_check.isChecked():
            output_formats.append('markdown')
        if self.html_check.isChecked():
            output_formats.append('html')
        if self.pdf_check.isChecked():
            output_formats.append('pdf')
        
        if not output_formats:
            QMessageBox.warning(self, 'è­¦å‘Š', 'è¯·è‡³å°‘é€‰æ‹©ä¸€ç§è¾“å‡ºæ ¼å¼!')
            return
        
        # ç¡®å®šå¹³å°
        platform_map = {
            0: 'runoob',
            1: 'csdn',
            2: 'gitbook'
        }
        platform = platform_map.get(self.platform_combo.currentIndex(), 'runoob')
        
        # ä¿å­˜è®¾ç½®
        self.save_settings()
        
        # åˆ›å»ºçˆ¬è™«çº¿ç¨‹
        self.crawler_thread = CrawlerThread()
        self.crawler_thread.url = url
        self.crawler_thread.platform = platform
        self.crawler_thread.output_dir = self.output_dir_input.text()
        self.crawler_thread.output_formats = output_formats
        self.crawler_thread.download_images = self.download_img_check.isChecked()
        self.crawler_thread.aggregate_mode = self.aggregate_check.isChecked()
        
        # è¿æ¥ä¿¡å·
        self.crawler_thread.progress_signal.connect(self.log_message)
        self.crawler_thread.finished_signal.connect(self.on_crawl_finished)
        
        # æ›´æ–°UI
        self.start_btn.setEnabled(False)
        self.stop_btn.setEnabled(True)
        self.log_text.clear()
        self.statusBar().showMessage('æ­£åœ¨æå–...')
        
        # å¯åŠ¨çº¿ç¨‹
        self.crawler_thread.start()
    
    def stop_crawl(self):
        if self.crawler_thread and self.crawler_thread.isRunning():
            self.crawler_thread.stop()
            self.log_message("â¹ï¸ ç”¨æˆ·åœæ­¢æ“ä½œ")
            self.statusBar().showMessage('å·²åœæ­¢')
    
    def on_crawl_finished(self, success: bool, message: str):
        self.log_message(message)
        self.start_btn.setEnabled(True)
        self.stop_btn.setEnabled(False)
        
        if success:
            self.statusBar().showMessage('å®Œæˆ!')
            QMessageBox.information(self, 'å®Œæˆ', message)
        else:
            self.statusBar().showMessage('å¤±è´¥')
            QMessageBox.critical(self, 'é”™è¯¯', message)
    
    def save_settings(self):
        self.settings.setValue('url', self.url_input.text())
        self.settings.setValue('platform', self.platform_combo.currentIndex())
        self.settings.setValue('output_dir', self.output_dir_input.text())
        self.settings.setValue('markdown', self.markdown_check.isChecked())
        self.settings.setValue('html', self.html_check.isChecked())
        self.settings.setValue('pdf', self.pdf_check.isChecked())
        self.settings.setValue('download_images', self.download_img_check.isChecked())
        self.settings.setValue('aggregate', self.aggregate_check.isChecked())
    
    def load_settings(self):
        self.url_input.setText(self.settings.value('url', ''))
        self.platform_combo.setCurrentIndex(int(self.settings.value('platform', 0)))
        self.output_dir_input.setText(self.settings.value('output_dir', './output'))
        self.markdown_check.setChecked(self.settings.value('markdown', True, type=bool))
        self.html_check.setChecked(self.settings.value('html', True, type=bool))
        self.pdf_check.setChecked(self.settings.value('pdf', True, type=bool))
        self.download_img_check.setChecked(self.settings.value('download_images', True, type=bool))
        self.aggregate_check.setChecked(self.settings.value('aggregate', True, type=bool))
    
    def closeEvent(self, event):
        if self.crawler_thread and self.crawler_thread.isRunning():
            reply = QMessageBox.question(
                self, 'ç¡®è®¤é€€å‡º',
                'çˆ¬è™«æ­£åœ¨è¿è¡Œï¼Œç¡®å®šè¦é€€å‡ºå—ï¼Ÿ',
                QMessageBox.Yes | QMessageBox.No
            )
            if reply == QMessageBox.Yes:
                self.crawler_thread.stop()
                self.crawler_thread.wait(3000)
                event.accept()
            else:
                event.ignore()
        else:
            event.accept()


# ======================== ä¸»ç¨‹åºå…¥å£ ========================

def main():
    app = QApplication(sys.argv)
    app.setStyle('Fusion')  # ä½¿ç”¨Fusionæ ·å¼
    
    # è®¾ç½®å…¨å±€å­—ä½“
    font = QFont('Microsoft YaHei', 10)
    app.setFont(font)
    
    window = MainWindow()
    window.show()
    
    sys.exit(app.exec_())


if __name__ == '__main__':
    main()